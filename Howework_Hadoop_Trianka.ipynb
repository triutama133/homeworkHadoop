{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QWiaeupxCpUtMo04nmNW6-n5Jx_c0CDK",
      "authorship_tag": "ABX9TyP38uTdRH54WGkk9owagZ2x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/triutama133/homeworkHadoop/blob/main/Howework_Hadoop_Trianka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update package list\n",
        "!apt-get update -y\n",
        "\n",
        "# Install OpenJDK 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Verify installation\n",
        "!java -version\n",
        "\n",
        "!readlink -f /usr/bin/java | sed \"s:/bin/java::\"\n",
        "\n",
        "!export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
        "!export PATH=$PATH:$JAVA_HOME/bin\n",
        "\n",
        "!echo $JAVA_HOME\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghPIQ2n4Th4W",
        "outputId": "f9fe00d4-fa33-4b21-cb59-3850768fd11a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,553 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,173 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,449 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,134 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,423 kB]\n",
            "Fetched 9,999 kB in 2s (5,144 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "openjdk version \"11.0.24\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n",
            "/usr/lib/jvm/java-11-openjdk-amd64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "\n",
        "# Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "\n",
        "# Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz\n",
        "\n",
        "# Copying the hadoop files to /usr/local\n",
        "!sudo mv hadoop-3.2.3 /usr/local/hadoop\n",
        "\n",
        "# Setting environment variables\n",
        "import os\n",
        "\n",
        "os.environ['HADOOP_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['HADOOP_INSTALL'] = '/usr/local/hadoop'\n",
        "os.environ['HADOOP_MAPRED_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['HADOOP_COMMON_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['HADOOP_HDFS_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['YARN_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['PATH'] = os.environ['PATH'] + ':' + os.environ['HADOOP_HOME'] + '/bin'\n",
        "os.environ['PATH'] = os.environ['PATH'] + ':' + os.environ['HADOOP_HOME'] + '/sbin'\n",
        "\n",
        "# Verifying Hadoop installation\n",
        "!hadoop version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sisTsuLHS10M",
        "outputId": "d33d1fdf-bafc-4fa0-b680-b14867ab7bf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.2.3\n",
            "Source code repository https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131\n",
            "Compiled by ubuntu on 2022-03-20T01:18Z\n",
            "Compiled with protoc 2.5.0\n",
            "From source with checksum 39bb14faec14b3aa25388a6d7c345fe8\n",
            "This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.3.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8VqycYRRcSxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faad5a64-eff5-4864-c619-ed97b3f0ded8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-17 15:17:21,736 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output_word_count\n",
            "2024-08-17 15:17:24,332 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-17 15:17:24,432 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-17 15:17:24,432 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-17 15:17:24,450 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-17 15:17:24,664 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-08-17 15:17:24,687 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-17 15:17:24,868 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1875541513_0001\n",
            "2024-08-17 15:17:24,868 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-17 15:17:25,053 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-17 15:17:25,055 INFO mapreduce.Job: Running job: job_local1875541513_0001\n",
            "2024-08-17 15:17:25,063 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-17 15:17:25,065 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-08-17 15:17:25,070 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 15:17:25,070 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 15:17:25,118 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-17 15:17:25,123 INFO mapred.LocalJobRunner: Starting task: attempt_local1875541513_0001_m_000000_0\n",
            "2024-08-17 15:17:25,150 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 15:17:25,150 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 15:17:25,173 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-17 15:17:25,182 INFO mapred.MapTask: Processing split: file:/content/pembukaan_uud1945.txt:0+1417\n",
            "2024-08-17 15:17:25,194 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-08-17 15:17:25,284 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-17 15:17:25,284 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-17 15:17:25,284 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-17 15:17:25,284 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-17 15:17:25,284 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-17 15:17:25,287 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-17 15:17:25,293 INFO streaming.PipeMapRed: PipeMapRed exec [/content/././mapper.py]\n",
            "2024-08-17 15:17:25,300 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-08-17 15:17:25,300 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-08-17 15:17:25,301 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-08-17 15:17:25,301 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-08-17 15:17:25,302 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-08-17 15:17:25,302 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-08-17 15:17:25,303 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-08-17 15:17:25,303 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-17 15:17:25,303 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-08-17 15:17:25,304 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-08-17 15:17:25,304 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-08-17 15:17:25,305 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-08-17 15:17:25,339 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 15:17:25,365 INFO streaming.PipeMapRed: Records R/W=8/1\n",
            "2024-08-17 15:17:25,372 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-17 15:17:25,373 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-17 15:17:25,377 INFO mapred.LocalJobRunner: \n",
            "2024-08-17 15:17:25,377 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-17 15:17:25,377 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-17 15:17:25,377 INFO mapred.MapTask: bufstart = 0; bufend = 1749; bufvoid = 104857600\n",
            "2024-08-17 15:17:25,377 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213688(104854752); length = 709/6553600\n",
            "2024-08-17 15:17:25,394 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-17 15:17:25,409 INFO mapred.Task: Task:attempt_local1875541513_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-17 15:17:25,412 INFO mapred.LocalJobRunner: Records R/W=8/1\n",
            "2024-08-17 15:17:25,412 INFO mapred.Task: Task 'attempt_local1875541513_0001_m_000000_0' done.\n",
            "2024-08-17 15:17:25,420 INFO mapred.Task: Final Counters for attempt_local1875541513_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=178093\n",
            "\t\tFILE: Number of bytes written=729201\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=8\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=1749\n",
            "\t\tMap output materialized bytes=2111\n",
            "\t\tInput split bytes=87\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=178\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1417\n",
            "2024-08-17 15:17:25,420 INFO mapred.LocalJobRunner: Finishing task: attempt_local1875541513_0001_m_000000_0\n",
            "2024-08-17 15:17:25,421 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-17 15:17:25,425 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-17 15:17:25,425 INFO mapred.LocalJobRunner: Starting task: attempt_local1875541513_0001_r_000000_0\n",
            "2024-08-17 15:17:25,435 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 15:17:25,435 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 15:17:25,435 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-17 15:17:25,444 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f76c055\n",
            "2024-08-17 15:17:25,447 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-17 15:17:25,474 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-17 15:17:25,480 INFO reduce.EventFetcher: attempt_local1875541513_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-17 15:17:25,514 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1875541513_0001_m_000000_0 decomp: 2107 len: 2111 to MEMORY\n",
            "2024-08-17 15:17:25,518 INFO reduce.InMemoryMapOutput: Read 2107 bytes from map-output for attempt_local1875541513_0001_m_000000_0\n",
            "2024-08-17 15:17:25,520 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2107, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2107\n",
            "2024-08-17 15:17:25,522 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-17 15:17:25,523 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 15:17:25,523 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-17 15:17:25,532 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-17 15:17:25,532 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2099 bytes\n",
            "2024-08-17 15:17:25,535 INFO reduce.MergeManagerImpl: Merged 1 segments, 2107 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-17 15:17:25,536 INFO reduce.MergeManagerImpl: Merging 1 files, 2111 bytes from disk\n",
            "2024-08-17 15:17:25,537 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-17 15:17:25,537 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-17 15:17:25,538 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2099 bytes\n",
            "2024-08-17 15:17:25,539 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 15:17:25,546 INFO streaming.PipeMapRed: PipeMapRed exec [/content/././reducer.py]\n",
            "2024-08-17 15:17:25,552 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-08-17 15:17:25,554 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-08-17 15:17:25,583 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 15:17:25,584 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 15:17:25,586 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 15:17:25,623 INFO streaming.PipeMapRed: Records R/W=178/1\n",
            "2024-08-17 15:17:25,628 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-17 15:17:25,631 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-17 15:17:25,631 INFO mapred.Task: Task:attempt_local1875541513_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-17 15:17:25,636 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 15:17:25,636 INFO mapred.Task: Task attempt_local1875541513_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-17 15:17:25,640 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1875541513_0001_r_000000_0' to file:/content/output_word_count\n",
            "2024-08-17 15:17:25,641 INFO mapred.LocalJobRunner: Records R/W=178/1 > reduce\n",
            "2024-08-17 15:17:25,641 INFO mapred.Task: Task 'attempt_local1875541513_0001_r_000000_0' done.\n",
            "2024-08-17 15:17:25,641 INFO mapred.Task: Final Counters for attempt_local1875541513_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=182347\n",
            "\t\tFILE: Number of bytes written=732531\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=114\n",
            "\t\tReduce shuffle bytes=2111\n",
            "\t\tReduce input records=178\n",
            "\t\tReduce output records=114\n",
            "\t\tSpilled Records=178\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1219\n",
            "2024-08-17 15:17:25,642 INFO mapred.LocalJobRunner: Finishing task: attempt_local1875541513_0001_r_000000_0\n",
            "2024-08-17 15:17:25,642 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-17 15:17:26,061 INFO mapreduce.Job: Job job_local1875541513_0001 running in uber mode : false\n",
            "2024-08-17 15:17:26,062 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-17 15:17:26,063 INFO mapreduce.Job: Job job_local1875541513_0001 completed successfully\n",
            "2024-08-17 15:17:26,071 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=360440\n",
            "\t\tFILE: Number of bytes written=1461732\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=8\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=1749\n",
            "\t\tMap output materialized bytes=2111\n",
            "\t\tInput split bytes=87\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=114\n",
            "\t\tReduce shuffle bytes=2111\n",
            "\t\tReduce input records=178\n",
            "\t\tReduce output records=114\n",
            "\t\tSpilled Records=356\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1417\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1219\n",
            "2024-08-17 15:17:26,071 INFO streaming.StreamJob: Output directory: /content/output_word_count\n",
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa\t1\n",
            "Indonesia\t12\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "Negara\t4\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan\t1\n",
            "Persatuan\t1\n",
            "Republik\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "bagi\t1\n",
            "bangsa\t3\n",
            "bebas\t1\n",
            "beradab\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "darah\t1\n",
            "daripada\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan\t1\n",
            "dipimpin\t1\n",
            "disusunlah\t1\n",
            "dunia\t2\n",
            "gerbang\t1\n",
            "hak\t1\n",
            "harus\t1\n",
            "hikmat\t1\n",
            "ialah\t1\n",
            "ikut\t1\n",
            "ini\t1\n",
            "itu\t4\n",
            "karena\t1\n",
            "ke\t1\n",
            "keadilan\t1\n",
            "kebangsaan\t1\n",
            "kebijaksanaan\t1\n",
            "kehidupan\t1\n",
            "keinginan\t1\n",
            "kemerdekaan\t3\n",
            "kemerdekaannya\t1\n",
            "kepada\t2\n",
            "kesejahteraan\t1\n",
            "ketertiban\t1\n",
            "luhur\t1\n",
            "maka\t3\n",
            "makmur\t1\n",
            "melaksanakan\t1\n",
            "melindungi\t1\n",
            "memajukan\t1\n",
            "membentuk\t1\n",
            "mencerdaskan\t1\n",
            "mengantarkan\t1\n",
            "menyatakan\t1\n",
            "merdeka\t1\n",
            "mewujudkan\t1\n",
            "oleh\t3\n",
            "penjajahan\t1\n",
            "perdamaian\t1\n",
            "pergerakan\t1\n",
            "perikeadilan\t1\n",
            "perikemanusiaan\t1\n",
            "perjuangan\t1\n",
            "pintu\t1\n",
            "rakhmat\t1\n",
            "rakyat\t4\n",
            "saat\t1\n",
            "sampailah\t1\n",
            "sebab\t1\n",
            "segala\t1\n",
            "segenap\t1\n",
            "selamat\t1\n",
            "seluruh\t2\n",
            "sentausa\t1\n",
            "serta\t1\n",
            "sesuai\t1\n",
            "sesungguhnya\t1\n",
            "sosial\t2\n",
            "suatu\t4\n",
            "supaya\t1\n",
            "susunan\t1\n",
            "telah\t1\n",
            "terbentuk\t1\n",
            "tidak\t1\n",
            "tumpah\t1\n",
            "umum\t1\n",
            "untuk\t2\n",
            "yang\t9\n"
          ]
        }
      ],
      "source": [
        "# Simpan mapper.py ke dalam file\n",
        "with open(\"mapper.py\", \"w\") as f:\n",
        "    f.write(\"\"\"#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    for word in line.strip().split():\n",
        "        cleaned_word = word.strip().strip('.,!?\":;()')\n",
        "        if cleaned_word:\n",
        "            print(f'{cleaned_word}\\\\t1')\n",
        "    \"\"\")\n",
        "\n",
        "# Simpan reducer.py ke dalam file\n",
        "with open(\"reducer.py\", \"w\") as f:\n",
        "    f.write(\"\"\"#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    word, count = line.split('\\\\t', 1)\n",
        "\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            print(f'{current_word}\\\\t{current_count}')\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "if current_word == word:\n",
        "    print(f'{current_word}\\\\t{current_count}')\n",
        "    \"\"\")\n",
        "\n",
        "!chmod +x mapper.py\n",
        "!chmod +x reducer.py\n",
        "\n",
        "!hdfs dfs -rm -r -f /content/output_word_count\n",
        "\n",
        "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "    -input /content/pembukaan_uud1945.txt \\\n",
        "    -output /content/output_word_count \\\n",
        "    -mapper ./mapper.py \\\n",
        "    -reducer ./reducer.py\n",
        "\n",
        "!cat /content/output_word_count/part-00000 > /content/output_word_count.txt\n",
        "!cat /content/output_word_count.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"\n",
        "!git config --global user.email \""
      ],
      "metadata": {
        "id": "_vQNpo4qaC61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git remote remove origin\n",
        "!git remote add origin https://github.com/triutama133/homeworkHadoop.git\n",
        "!git remote -v\n",
        "# Tambahkan remote repository dengan token\n"
      ],
      "metadata": {
        "id": "lJ9v-oczazqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menambahkan file ke staging area\n",
        "!git add mapper.py reducer.py output_word_count.txt pembukaan_uud1945.txt\n"
      ],
      "metadata": {
        "id": "mJARp3xia3sx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commit perubahan\n",
        "!git commit -m \"Add mapper, reducer scripts and output word count\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX2H8vhba9G1",
        "outputId": "fe667e74-b3ee-4830-e1f1-3f0dee53c7eb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main (root-commit) 00a9b54] Add mapper, reducer scripts and output word count\n",
            " 4 files changed, 157 insertions(+)\n",
            " create mode 100755 mapper.py\n",
            " create mode 100644 output_word_count.txt\n",
            " create mode 100644 pembukaan_uud1945.txt\n",
            " create mode 100755 reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push perubahan ke GitHub\n",
        "!git branch --set-upstream-to=origin/main main\n",
        "!git pull origin main\n",
        "!git push -f origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfshhrHlbFCB",
        "outputId": "1e0fd2b4-da78-40c9-857d-934adef4c1dd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
            "From https://github.com/triutama133/homeworkHadoop\n",
            " * branch            main       -> FETCH_HEAD\n",
            "fatal: Not possible to fast-forward, aborting.\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (6/6), done.\n",
            "Writing objects: 100% (6/6), 1.88 KiB | 1.88 MiB/s, done.\n",
            "Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/triutama133/homeworkHadoop.git\n",
            " + 27d2071...00a9b54 main -> main (forced update)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout --orphan main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtoP-lpX8lKz",
        "outputId": "28d183ec-7cb3-4be1-a30c-9e3c24b47ee2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Switched to a new branch 'main'\n"
          ]
        }
      ]
    }
  ]
}